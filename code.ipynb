{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trabajo Final de Grado: Predicción de Precios de Acciones con Modelos de Aprendizaje Profundo\n",
        "\n",
        "Este proyecto aborda la predicción de precios de acciones utilizando técnicas avanzadas de aprendizaje automático y aprendizaje profundo, combinando modelos tradicionales con redes neuronales recurrentes. El objetivo principal es evaluar el rendimiento de diferentes enfoques para determinar su eficacia en el análisis de series temporales financieras.\n",
        "\n",
        "## Etapas del proyecto\n",
        "El trabajo sigue una metodología estructurada, dividida en las siguientes etapas:\n",
        "1. **Preprocesamiento de datos:** Limpieza, normalización y generación de indicadores técnicos como SMA, EMA y RSI, esenciales para enriquecer los datos utilizados por los modelos.\n",
        "2. **Visualización y análisis exploratorio:** Creación de gráficos que faciliten la comprensión de los datos y los patrones subyacentes.\n",
        "3. **Implementación de modelos base:** Construcción de un modelo de regresión lineal como referencia inicial para comparar los resultados.\n",
        "4. **Implementación de modelos avanzados:** Diseño, entrenamiento y optimización de redes LSTM y GRU, incluyendo experimentos con diferentes configuraciones de hiperparámetros.\n",
        "\n",
        "## Librerías utilizadas\n",
        "- `pandas` y `numpy` para manipulación y análisis de datos.\n",
        "- `sklearn` para el modelo base y evaluación de métricas.\n",
        "- `tensorflow` para la construcción y entrenamiento de redes neuronales.\n",
        "- `matplotlib` para visualización de datos y resultados.\n",
        "\n",
        "## Dataset\n",
        "El dataset utilizado (`aapl.csv`) contiene datos históricos de precios de acciones de Apple. Este archivo fue descargado de una fuente pública confiable y será preprocesado en las etapas iniciales para garantizar la calidad y consistencia de los datos.\n",
        "\n",
        "## Estructura del Notebook\n",
        "- **Sección 1:** Importación y configuración de librerías.\n",
        "- **Sección 2:** Preprocesamiento de datos.\n",
        "- **Sección 3:** Generación de indicadores técnicos y normalización final.\n",
        "- **Sección 4:** Normalización final de indicadores.\n",
        "- **Sección 5:** Visualización y análisis exploratorio.\n",
        "- **Sección 6:** Implementación de modelos base (Regresión Lineal).\n",
        "- **Sección 7:** Implementación de modelos avanzados (LSTM y GRU).\n",
        "- **Sección 8:** Optimización de hiperparámetros.\n",
        "\n",
        "**Nota:** Todos los archivos generados durante este proceso se almacenarán en la carpeta `data/` para mantener una estructura de trabajo organizada y reproducible.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Importación y configuración de librerías\n",
        "En este primer bloque, se importan las librerías necesarias para todo el proyecto. Estas herramientas son fundamentales para manejar los datos, construir modelos y evaluar resultados.\n",
        "\n",
        "- `pandas` y `numpy` se usarán para la manipulación y análisis de datos.\n",
        "- `tensorflow` será la base para construir y entrenar redes neuronales.\n",
        "- `sklearn` se empleará para la implementación del modelo de regresión lineal y la evaluación de métricas.\n",
        "- `matplotlib` se utilizará para la visualización de datos e indicadores técnicos.\n",
        "\n",
        "Además, se configura el entorno de trabajo estableciendo rutas estándar para cargar y guardar archivos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, GRU, Dense\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "# Importación de librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import product\n",
        "\n",
        "# Configuración de rutas de archivos\n",
        "DATA_PATH = 'data/'  # Carpeta para datasets y archivos procesados\n",
        "print(f\"Archivos se leerán de: {DATA_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocesamiento de datos\n",
        "En este bloque, se realiza el preprocesamiento inicial del dataset de precios históricos de acciones (`aapl.csv`). Los pasos principales incluyen:\n",
        "\n",
        "1. **Cargar los datos:** Leer el archivo CSV y configurar la columna `Date` como índice.\n",
        "2. **Conversión de tipos:** Convertir la columna de fechas al formato adecuado.\n",
        "3. **Limpieza de datos:** Identificar y reportar valores nulos.\n",
        "4. **Normalización:** Aplicar Min-Max Scaling para escalar todas las columnas al rango [0, 1].\n",
        "\n",
        "El resultado de este proceso es un nuevo archivo llamado `aapl_normalized.csv`, que será la base para las siguientes etapas del análisis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Cargar el dataset original\n",
        "file_path = DATA_PATH + 'aapl.csv'\n",
        "apple_data = pd.read_csv(file_path)\n",
        "\n",
        "# Convertir la columna 'Date' al formato de fecha y establecerla como índice\n",
        "apple_data['Date'] = pd.to_datetime(apple_data['Date'])\n",
        "apple_data.set_index('Date', inplace=True)\n",
        "\n",
        "# Comprobar valores nulos\n",
        "missing_values = apple_data.isnull().sum()\n",
        "print(\"Valores nulos por columna:\")\n",
        "print(missing_values)\n",
        "\n",
        "# Normalizar columnas de precios y volumen\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(apple_data)\n",
        "normalized_data = pd.DataFrame(scaled_data, columns=apple_data.columns, index=apple_data.index)\n",
        "\n",
        "# Guardar datos normalizados\n",
        "normalized_file_path = DATA_PATH + 'aapl_normalized.csv'\n",
        "normalized_data.to_csv(normalized_file_path)\n",
        "print(f\"Datos normalizados guardados en: {normalized_file_path}\")\n",
        "\n",
        "# Visualización de los datos procesados\n",
        "print(\"Primeras filas de los datos normalizados:\")\n",
        "print(normalized_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Generación de indicadores técnicos\n",
        "En este bloque, se generan indicadores técnicos clave que enriquecen el dataset y ayudan a capturar patrones relevantes. Los indicadores incluyen:\n",
        "\n",
        "1. **Media Móvil Simple (SMA):** Calculada para ventanas de 5, 10 y 20 días.\n",
        "2. **Media Móvil Exponencial (EMA):** Calculada para una ventana de 10 días.\n",
        "3. **Índice de Fuerza Relativa (RSI):** Calculado para una ventana de 14 días.\n",
        "4. **Volatilidad histórica:** Desviación estándar de los precios de cierre para una ventana de 10 días.\n",
        "5. **Rango Verdadero Promedio (ATR):** Indicador de volatilidad calculado para 14 días.\n",
        "\n",
        "El archivo generado se guarda como `aapl_with_indicators.csv` y será normalizado en el siguiente paso.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Cargar los datos normalizados\n",
        "normalized_file_path = DATA_PATH + 'aapl_normalized.csv'\n",
        "normalized_data = pd.read_csv(normalized_file_path, index_col='Date', parse_dates=True)\n",
        "\n",
        "# Calcular indicadores técnicos\n",
        "# 1. Media móvil simple (SMA) para ventanas de 5, 10 y 20 días\n",
        "normalized_data['SMA_5'] = normalized_data['Close'].rolling(window=5).mean()\n",
        "normalized_data['SMA_10'] = normalized_data['Close'].rolling(window=10).mean()\n",
        "normalized_data['SMA_20'] = normalized_data['Close'].rolling(window=20).mean()\n",
        "\n",
        "# 2. Media móvil exponencial (EMA) para una ventana de 10 días\n",
        "normalized_data['EMA_10'] = normalized_data['Close'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "# 3. Índice de Fuerza Relativa (RSI) para 14 días\n",
        "delta = normalized_data['Close'].diff()\n",
        "gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "rs = gain / loss\n",
        "normalized_data['RSI_14'] = 100 - (100 / (1 + rs))\n",
        "\n",
        "# 4. Volatilidad histórica (desviación estándar de los precios de cierre en 10 días)\n",
        "normalized_data['Volatility_10'] = normalized_data['Close'].rolling(window=10).std()\n",
        "\n",
        "# 5. Rango verdadero promedio (ATR) para 14 días\n",
        "high_low = normalized_data['High'] - normalized_data['Low']\n",
        "high_close = (normalized_data['High'] - normalized_data['Close'].shift()).abs()\n",
        "low_close = (normalized_data['Low'] - normalized_data['Close'].shift()).abs()\n",
        "true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
        "normalized_data['ATR_14'] = true_range.rolling(window=14).mean()\n",
        "\n",
        "# Guardar los datos con indicadores técnicos\n",
        "indicators_file_path = DATA_PATH + 'aapl_with_indicators.csv'\n",
        "normalized_data.to_csv(indicators_file_path)\n",
        "print(f\"Datos con indicadores técnicos guardados en: {indicators_file_path}\")\n",
        "\n",
        "# Visualización de los datos con indicadores añadidos\n",
        "print(\"Primeras filas con indicadores técnicos añadidos:\")\n",
        "print(normalized_data.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Normalización final de indicadores\n",
        "Aquí se realiza un paso adicional de preprocesamiento para garantizar que todos los indicadores técnicos estén en el mismo rango. Los pasos incluyen:\n",
        "\n",
        "1. **Eliminar valores nulos:** Remover filas generadas durante el cálculo de indicadores debido a ventanas de tiempo.\n",
        "2. **Normalizar indicadores seleccionados:** Aplicar Min-Max Scaling a las columnas `SMA_5`, `EMA_10`, `RSI_14`, etc.\n",
        "3. **Guardar el dataset final:** Crear un archivo llamado `aapl_normalized_with_indicators.csv`.\n",
        "\n",
        "Este archivo será utilizado en las siguientes secciones para construir y entrenar los modelos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Cargar los datos generados previamente con indicadores técnicos\n",
        "indicators_file_path = DATA_PATH + 'aapl_with_indicators.csv'\n",
        "data_with_indicators = pd.read_csv(indicators_file_path, index_col='Date', parse_dates=True)\n",
        "\n",
        "# 1. Eliminar filas con valores nulos generados por indicadores técnicos\n",
        "data_with_indicators.dropna(inplace=True)\n",
        "\n",
        "# 2. Normalización de los indicadores técnicos\n",
        "indicators_to_normalize = ['SMA_5', 'SMA_10', 'SMA_20', 'EMA_10',\n",
        "                           'RSI_14', 'Volatility_10', 'ATR_14']\n",
        "\n",
        "# Aplicar escalado Min-Max\n",
        "scaler = MinMaxScaler()\n",
        "normalized_values = scaler.fit_transform(data_with_indicators[indicators_to_normalize])\n",
        "data_with_indicators[indicators_to_normalize] = normalized_values\n",
        "\n",
        "# Guardar los datos normalizados en un nuevo archivo CSV\n",
        "final_data_file_path = DATA_PATH + 'aapl_normalized_with_indicators.csv'\n",
        "data_with_indicators.to_csv(final_data_file_path)\n",
        "\n",
        "print(f\"Datos finalizados guardados en: {final_data_file_path}\")\n",
        "print(\"Primeras filas del conjunto final de datos:\")\n",
        "print(data_with_indicators.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Visualización y análisis exploratorio.\n",
        "Antes de entrenar los modelos, se realiza una exploración visual de los datos. Este paso incluye:\n",
        "\n",
        "1. **Gráficas de precios históricos:** Visualización de la columna `Close`.\n",
        "2. **Indicadores técnicos:** Superposición de indicadores como `SMA_5` y `RSI_14` para identificar tendencias y patrones.\n",
        "\n",
        "Las visualizaciones ayudan a entender el comportamiento del mercado y a justificar el uso de ciertos modelos predictivos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Cargar los datos procesados\n",
        "visualization_data_path = DATA_PATH + 'aapl_normalized_with_indicators.csv'\n",
        "data = pd.read_csv(visualization_data_path, index_col='Date', parse_dates=True)\n",
        "\n",
        "# Configurar el tamaño del gráfico\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Graficar columnas de precios originales (Close)\n",
        "plt.plot(data.index, data['Close'], label='Precio de Cierre (Close)', color='blue', alpha=0.8)\n",
        "\n",
        "# Graficar indicadores técnicos (SMA_5, RSI_14)\n",
        "plt.plot(data.index, data['SMA_5'], label='Media Móvil Simple (SMA_5)', color='orange', alpha=0.7)\n",
        "plt.plot(data.index, data['RSI_14'], label='Índice de Fuerza Relativa (RSI_14)', color='green', alpha=0.7)\n",
        "\n",
        "# Añadir título y etiquetas\n",
        "plt.title('Visualización de Precios e Indicadores Técnicos', fontsize=16)\n",
        "plt.xlabel('Fecha', fontsize=12)\n",
        "plt.ylabel('Valor Normalizado', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Guardar el gráfico como imagen\n",
        "output_image_path = DATA_PATH + 'aapl_visualization.png'\n",
        "plt.savefig(output_image_path, dpi=300, bbox_inches='tight')  # Guardar con alta calidad\n",
        "print(f\"Gráfico guardado como imagen en: {output_image_path}\")\n",
        "\n",
        "# Mostrar el gráfico\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Implementación del Modelo Base (Regresión Lineal)\n",
        "Se construye un modelo de regresión lineal como línea base para comparar el rendimiento con los modelos avanzados. Los pasos incluyen:\n",
        "\n",
        "1. **Crear el conjunto de entrenamiento y prueba:** Dividir los datos en un 80%-20%.\n",
        "2. **Entrenar el modelo:** Utilizar el conjunto de entrenamiento para ajustar el modelo.\n",
        "3. **Evaluar el modelo:** Calcular métricas como MSE y MAE para el conjunto de prueba.\n",
        "4. **Visualizar resultados:** Comparar valores reales y predicciones en una gráfica.\n",
        "\n",
        "Este modelo servirá como referencia inicial para evaluar la mejora introducida por los modelos LSTM y GRU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Crear la variable objetivo y las características\n",
        "data['Target'] = data['Close'].shift(-1)  # Precio de cierre del día siguiente\n",
        "data.dropna(inplace=True)  # Eliminar filas con valores faltantes debido al shift\n",
        "\n",
        "# Separar características (X) y variable objetivo (y)\n",
        "X = data.drop(columns=['Close', 'Target'])  # Excluir la variable objetivo\n",
        "y = data['Target']  # Precio de cierre del día siguiente\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento y prueba (80%-20%)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
        "\n",
        "# Entrenar el modelo de regresión lineal\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "print(f\"Error cuadrático medio (MSE): {mse}\")\n",
        "print(f\"Error absoluto medio (MAE): {mae}\")\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.plot(y_test.index, y_test, label='Valores reales', color='blue')\n",
        "plt.plot(y_test.index, y_pred, label='Predicciones', color='orange', linestyle='--')\n",
        "plt.title('Predicción del precio de cierre con Regresión Lineal')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Precio de Cierre (Normalizado)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(DATA_PATH + 'linear_regression_baseline.png', dpi=300, bbox_inches='tight')  # Guardar gráfico\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Implementación de modelos avanzados (LSTM y GRU)\n",
        "En esta sección, se implementan los modelos LSTM y GRU. Los pasos incluyen:\n",
        "\n",
        "1. **Crear secuencias temporales:** Dividir los datos en ventanas de tiempo para capturar dependencias temporales.\n",
        "2. **Configurar y entrenar los modelos:** Ajustar parámetros como el tamaño de la ventana, número de unidades y épocas de entrenamiento.\n",
        "3. **Evaluar los modelos:** Comparar las métricas MSE y MAE entre los modelos LSTM y GRU.\n",
        "4. **Visualizar las curvas de pérdida:** Analizar el comportamiento durante el entrenamiento para identificar problemas de sobreajuste.\n",
        "\n",
        "Se prueban configuraciones con 5, 7 y 10 épocas para analizar la estabilidad y precisión de los modelos.\n",
        "\n",
        "**Nota:** El código proporcionado en este bloque está configurado para entrenar los modelos durante **10 épocas**, pero puede ajustarse fácilmente modificando el parámetro `epochs` para adaptarse a diferentes experimentos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Configuración del tamaño de la ventana\n",
        "window_size = 10\n",
        "\n",
        "# Crear secuencias para las redes neuronales\n",
        "def create_sequences(data, window_size):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        seq = data.iloc[i:i + window_size].values\n",
        "        target = data.iloc[i + window_size][\"Close\"]  # Predicción del precio de cierre\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# Crear secuencias a partir de las columnas del dataset\n",
        "X, y = create_sequences(data, window_size)\n",
        "\n",
        "# División en entrenamiento y prueba\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Convertir los datos en tf.data.Dataset para entrenamiento y prueba\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(64)\n",
        "\n",
        "# Verificación de dimensiones\n",
        "print(f\"Tamaño de X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"Tamaño de X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "# Función para crear modelo LSTM\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, input_shape=input_shape))  # 50 unidades LSTM\n",
        "    model.add(Dense(1, activation='linear'))  # Capa densa con una neurona de salida (predicción de cierre)\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')  # Optimización con Adam y MSE\n",
        "    return model\n",
        "\n",
        "# Función para crear modelo GRU\n",
        "def create_gru_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(50, input_shape=input_shape))  # 50 unidades GRU\n",
        "    model.add(Dense(1, activation='linear'))  # Capa densa con una neurona de salida (predicción de cierre)\n",
        "    model.compile(optimizer=Adam(), loss='mean_squared_error')  # Optimización con Adam y MSE\n",
        "    return model\n",
        "\n",
        "# Ajustar el modelo LSTM\n",
        "print(\"Entrenando el modelo LSTM para 10 épocas...\")\n",
        "lstm_model = create_lstm_model((X_train.shape[1], X_train.shape[2]))  # (10, 13)\n",
        "lstm_history = lstm_model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n",
        "\n",
        "# Ajustar el modelo GRU\n",
        "print(\"Entrenando el modelo GRU para 10 épocas...\")\n",
        "gru_model = create_gru_model((X_train.shape[1], X_train.shape[2]))  # (10, 13)\n",
        "gru_history = gru_model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n",
        "\n",
        "# Evaluar los modelos\n",
        "print(\"Evaluando el modelo LSTM para 10 épocas...\")\n",
        "lstm_mse = lstm_model.evaluate(test_dataset)\n",
        "print(f\"Error cuadrático medio (MSE) del modelo LSTM: {lstm_mse}\")\n",
        "\n",
        "print(\"Evaluando el modelo GRU para 10 épocas...\")\n",
        "gru_mse = gru_model.evaluate(test_dataset)\n",
        "print(f\"Error cuadrático medio (MSE) del modelo GRU: {gru_mse}\")\n",
        "\n",
        "# Calcular MAE manualmente\n",
        "lstm_predictions = lstm_model.predict(X_test)\n",
        "gru_predictions = gru_model.predict(X_test)\n",
        "\n",
        "mae_metric = tf.keras.metrics.MeanAbsoluteError()\n",
        "\n",
        "lstm_mae = mae_metric(y_test, lstm_predictions).numpy()\n",
        "gru_mae = mae_metric(y_test, gru_predictions).numpy()\n",
        "print(f\"Error absoluto medio (MAE) del modelo LSTM para 10 épocas: {lstm_mae}\")\n",
        "print(f\"Error absoluto medio (MAE) del modelo GRU para 10 épocas: {gru_mae}\")\n",
        "\n",
        "# Visualización de predicciones: Modelo LSTM\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(len(y_test)), y_test, label=\"Valores reales\", color=\"blue\", alpha=0.6)\n",
        "plt.plot(range(len(y_test)), lstm_predictions, label=\"Predicciones LSTM\", color=\"orange\", alpha=0.8)\n",
        "plt.title(\"Modelo LSTM: Predicciones vs. Valores reales\")\n",
        "plt.xlabel(\"Timestep\")\n",
        "plt.ylabel(\"Precio de cierre (normalizado)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualización de predicciones: Modelo GRU\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(len(y_test)), y_test, label=\"Valores reales\", color=\"blue\", alpha=0.6)\n",
        "plt.plot(range(len(y_test)), gru_predictions, label=\"Predicciones GRU\", color=\"green\", alpha=0.8)\n",
        "plt.title(\"Modelo GRU: Predicciones vs. Valores reales\")\n",
        "plt.xlabel(\"Timestep\")\n",
        "plt.ylabel(\"Precio de cierre (normalizado)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Guardar las curvas de pérdida\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(lstm_history.history['loss'], label=\"Pérdida (entrenamiento)\", color=\"blue\")\n",
        "plt.plot(lstm_history.history['val_loss'], label=\"Pérdida (validación)\", color=\"orange\")\n",
        "plt.title(\"Modelo LSTM: Evolución de la pérdida (10 épocas)\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Pérdida (MSE)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(gru_history.history['loss'], label=\"Pérdida (entrenamiento)\", color=\"blue\")\n",
        "plt.plot(gru_history.history['val_loss'], label=\"Pérdida (validación)\", color=\"green\")\n",
        "plt.title(\"Modelo GRU: Evolución de la pérdida (10 épocas)\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Pérdida (MSE)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Optimización de hiperparámetros\n",
        "Utilizando Grid Search, se optimizan parámetros clave para mejorar el rendimiento de los modelos avanzados. Los pasos incluyen:\n",
        "\n",
        "1. **Definir el espacio de búsqueda:** Especificar valores posibles para el tamaño de la ventana, número de unidades, tamaño de lote y tasa de aprendizaje.\n",
        "2. **Entrenar múltiples configuraciones:** Evaluar todas las combinaciones posibles utilizando el MSE como métrica principal.\n",
        "3. **Seleccionar la mejor configuración:** Identificar la combinación de hiperparámetros que minimiza el error en el conjunto de prueba.\n",
        "\n",
        "Los resultados se resumen en un DataFrame, ordenado por el MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Configuración del tamaño de la ventana y creación de secuencias\n",
        "def create_sequences(data, window_size):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - window_size):\n",
        "        seq = data.iloc[i:i + window_size].values\n",
        "        target = data.iloc[i + window_size][\"Close\"]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target)\n",
        "    return np.array(sequences), np.array(targets)\n",
        "\n",
        "# Parámetros de optimización\n",
        "window_sizes = [10, 20, 30]\n",
        "units_options = [32, 64, 128]\n",
        "batch_sizes = [32, 64]\n",
        "learning_rates = [0.001, 0.0005]\n",
        "\n",
        "# Resultados\n",
        "results = []\n",
        "\n",
        "# Grid Search\n",
        "for window_size, units, batch_size, learning_rate in product(window_sizes, units_options, batch_sizes, learning_rates):\n",
        "    # Crear secuencias para esta ventana\n",
        "    X, y = create_sequences(data, window_size)\n",
        "    train_size = int(len(X) * 0.8)\n",
        "    X_train, X_test = X[:train_size], X[train_size:]\n",
        "    y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "    # Crear el modelo LSTM\n",
        "    def create_lstm_model(input_shape, units, learning_rate):\n",
        "        model = Sequential()\n",
        "        model.add(LSTM(units, input_shape=input_shape))\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
        "        return model\n",
        "\n",
        "    # Crear el modelo GRU\n",
        "    def create_gru_model(input_shape, units, learning_rate):\n",
        "        model = Sequential()\n",
        "        model.add(GRU(units, input_shape=input_shape))\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n",
        "        return model\n",
        "\n",
        "    # Entrenar y evaluar LSTM\n",
        "    lstm_model = create_lstm_model((X_train.shape[1], X_train.shape[2]), units, learning_rate)\n",
        "    lstm_model.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=0)\n",
        "    lstm_mse = lstm_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    # Entrenar y evaluar GRU\n",
        "    gru_model = create_gru_model((X_train.shape[1], X_train.shape[2]), units, learning_rate)\n",
        "    gru_model.fit(X_train, y_train, epochs=5, batch_size=batch_size, verbose=0)\n",
        "    gru_mse = gru_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    # Guardar resultados\n",
        "    results.append({\n",
        "        \"model\": \"LSTM\",\n",
        "        \"window_size\": window_size,\n",
        "        \"units\": units,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"mse\": lstm_mse\n",
        "    })\n",
        "    results.append({\n",
        "        \"model\": \"GRU\",\n",
        "        \"window_size\": window_size,\n",
        "        \"units\": units,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"mse\": gru_mse\n",
        "    })\n",
        "\n",
        "# Convertir resultados a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Mostrar las mejores configuraciones\n",
        "print(\"Mejores configuraciones:\")\n",
        "print(results_df.sort_values(by=\"mse\").head(10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
